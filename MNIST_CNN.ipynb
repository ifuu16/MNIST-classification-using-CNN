{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f80a945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8cbea091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the hyperparameters\n",
    "BUFFER_SIZE = 70_000\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fcf5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the dataset\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info = True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = mnist_dataset['train'],  mnist_dataset['test']\n",
    "\n",
    "\n",
    "#Scaling the dataset. Fist cast the image to avoid value error\n",
    "def scale(image,label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 225.\n",
    "    \n",
    "    return image, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3715952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "#creating the number of validation dataset from the training dataset\n",
    "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
    "\n",
    "#convert number of the validation sample to an integer to prevent potential issue\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "#get the number data point in our  test samples\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "#Shuffle the training and validation dataset\n",
    "shuffle_train_and_validation_data = train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "\n",
    "#create the train data\n",
    "train_data = shuffle_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "#create the validation data\n",
    "validation_data = shuffle_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "\n",
    "#batching the training data\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "#No need to batch the validation and test data(we are not backpropagating on them ). Hence we will take all at once]\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35f4361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model\n",
    "model = Sequential([\n",
    "        Conv2D(50, 5, activation= 'relu', input_shape=(28,28,1)),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Conv2D(50, 3, activation= 'relu'),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d37cebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "___________________________________________________________________________\n",
      " Layer (type)                    Output Shape                  Param #     \n",
      "===========================================================================\n",
      " conv2d_10 (Conv2D)              (None, 24, 24, 50)            1300        \n",
      "                                                                           \n",
      " max_pooling2d_10 (MaxPooling2D)  (None, 12, 12, 50)           0           \n",
      "                                                                           \n",
      " conv2d_11 (Conv2D)              (None, 10, 10, 50)            22550       \n",
      "                                                                           \n",
      " max_pooling2d_11 (MaxPooling2D)  (None, 5, 5, 50)             0           \n",
      "                                                                           \n",
      " flatten_5 (Flatten)             (None, 1250)                  0           \n",
      "                                                                           \n",
      " dense_5 (Dense)                 (None, 10)                    12510       \n",
      "                                                                           \n",
      "===========================================================================\n",
      "Total params: 36,360\n",
      "Trainable params: 36,360\n",
      "Non-trainable params: 0\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(line_length=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f49cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe the loss function\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#Compile the model\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0fbc39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 2s 10ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of unknown and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m val_predict_raw\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(val_images)\n\u001b[0;32m     11\u001b[0m val_predict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(val_predict_raw, axis \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_predict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Define class labels\u001b[39;00m\n\u001b[0;32m     17\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass 1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass 2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass 3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass 4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass 5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass 6\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass 7\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass 8\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass 9\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:326\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    232\u001b[0m     {\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    243\u001b[0m ):\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of unknown and multiclass targets"
     ]
    }
   ],
   "source": [
    "#enumerate the runs\n",
    "#log_dir = f'./logs/{run-1}'\n",
    "\n",
    "# Create a Confusion Matrix\n",
    "\n",
    "for images, labels in validation_data:\n",
    "    val_images = images.numpy()\n",
    "    val_labels = labels.numpy()\n",
    "    \n",
    "val_predict_raw= model.predict(val_images)\n",
    "val_predict = np.argmax(val_predict_raw, axis =1)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(val_images, val_predict)\n",
    "\n",
    "# Define class labels\n",
    "class_labels = [\"Class 0\", \"Class 1\", \"Class 2\",\"Class 3\",\"Class 4\",\"Class 5\",\"Class 6\",\"Class 7\",\"Class 8\",\"Class 9\"]\n",
    "\n",
    "# Plot the confusion matrix using Seaborn\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.set(font_scale=1.2)  # Adjust the font size\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", square=True,\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels)\n",
    "\n",
    "\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "# disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f410c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5477e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop the model from overfitting ie whenever the validation loss increases\n",
    "#the code tells the model to stop when the val_loss starts to increase in two subsequent epics\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor= 'val_loss',\n",
    "    mode = 'auto',\n",
    "    min_delta = 0,\n",
    "    patience = 2,\n",
    "    verbose = 0,\n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8387846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the current date and time as a string\n",
    "current_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "#Create a log directory based on the current date and time\n",
    "\n",
    "log_dir = f'./logs/{current_datetime}'\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "565cb469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "422/422 - 45s - loss: 0.2633 - accuracy: 0.9249 - val_loss: 0.0903 - val_accuracy: 0.9740 - 45s/epoch - 106ms/step\n",
      "Epoch 2/20\n",
      "422/422 - 44s - loss: 0.0725 - accuracy: 0.9787 - val_loss: 0.0528 - val_accuracy: 0.9847 - 44s/epoch - 104ms/step\n",
      "Epoch 3/20\n",
      "422/422 - 43s - loss: 0.0528 - accuracy: 0.9840 - val_loss: 0.0373 - val_accuracy: 0.9905 - 43s/epoch - 101ms/step\n",
      "Epoch 4/20\n",
      "422/422 - 44s - loss: 0.0432 - accuracy: 0.9871 - val_loss: 0.0348 - val_accuracy: 0.9890 - 44s/epoch - 104ms/step\n",
      "Epoch 5/20\n",
      "422/422 - 43s - loss: 0.0371 - accuracy: 0.9891 - val_loss: 0.0302 - val_accuracy: 0.9912 - 43s/epoch - 101ms/step\n",
      "Epoch 6/20\n",
      "422/422 - 43s - loss: 0.0329 - accuracy: 0.9901 - val_loss: 0.0335 - val_accuracy: 0.9882 - 43s/epoch - 102ms/step\n",
      "Epoch 7/20\n",
      "422/422 - 44s - loss: 0.0286 - accuracy: 0.9910 - val_loss: 0.0240 - val_accuracy: 0.9928 - 44s/epoch - 104ms/step\n",
      "Epoch 8/20\n",
      "422/422 - 44s - loss: 0.0239 - accuracy: 0.9924 - val_loss: 0.0167 - val_accuracy: 0.9950 - 44s/epoch - 103ms/step\n",
      "Epoch 9/20\n",
      "422/422 - 44s - loss: 0.0211 - accuracy: 0.9933 - val_loss: 0.0178 - val_accuracy: 0.9942 - 44s/epoch - 105ms/step\n",
      "Epoch 10/20\n",
      "422/422 - 44s - loss: 0.0196 - accuracy: 0.9941 - val_loss: 0.0140 - val_accuracy: 0.9948 - 44s/epoch - 105ms/step\n",
      "Epoch 11/20\n",
      "422/422 - 44s - loss: 0.0174 - accuracy: 0.9946 - val_loss: 0.0135 - val_accuracy: 0.9967 - 44s/epoch - 105ms/step\n",
      "Epoch 12/20\n",
      "422/422 - 44s - loss: 0.0155 - accuracy: 0.9949 - val_loss: 0.0183 - val_accuracy: 0.9940 - 44s/epoch - 104ms/step\n",
      "Epoch 13/20\n",
      "422/422 - 43s - loss: 0.0141 - accuracy: 0.9957 - val_loss: 0.0053 - val_accuracy: 0.9993 - 43s/epoch - 101ms/step\n",
      "Epoch 14/20\n",
      "422/422 - 44s - loss: 0.0116 - accuracy: 0.9964 - val_loss: 0.0069 - val_accuracy: 0.9982 - 44s/epoch - 104ms/step\n",
      "Epoch 15/20\n",
      "422/422 - 44s - loss: 0.0101 - accuracy: 0.9967 - val_loss: 0.0089 - val_accuracy: 0.9965 - 44s/epoch - 103ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b1c9c0bd30>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(\n",
    "    train_data,\n",
    "    epochs = NUM_EPOCHS,\n",
    "    callbacks = [tensorboard_callback, early_stopping],\n",
    "    validation_data = validation_data,\n",
    "    verbose =2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fd383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the model\n",
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "\n",
    "print(f'Test Loss: {round(test_loss,2)}. Test Accuracy:{round(test_accuracy*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cccc936",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot the image and result\n",
    "\n",
    "\n",
    "\n",
    "# Get and display the first n images from the training dataset. n=2\n",
    "num_images_to_display = 1\n",
    "for image, label in test_data.take(num_images_to_display):\n",
    "    sample_image = image.numpy()# Convert to numpy array and remove singleton dimensions\n",
    "    sample_label = label.numpy()\n",
    "\n",
    "original_images = sample_image.reshape(10000, 28, 28)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "image_number_to_display = 8\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(sample_image[image_number_to_display], cmap='gray', aspect=\"auto\")\n",
    "plt.title(f'Label: {sample_label[image_number_to_display]}')\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain the model's prediction(logits)\n",
    "\n",
    "#Using slicing is important because tensorflow expects inputs to be in batches\n",
    "predictions = model.predict(sample_image[image_number_to_display:image_number_to_display+1])\n",
    "\n",
    "#Convert the predictions into probabilities and to percentage\n",
    "probabilities = (tf.nn.softmax(predictions).numpy()) *100\n",
    "\n",
    "#Create a bar chart to plot the probabilities for each class\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(x=[1,2,3,4,5,6,7,8,9,10], height=probabilities[0], tick_label=[\"0\", \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956201a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIsualize the Tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f5ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a90856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
